{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ab9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/vector_store.ipynb - Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import hashlib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load data\n",
    "data_path = Path('../data/processed/filtered_complaints.csv')\n",
    "\n",
    "if not data_path.exists():\n",
    "    print(f\"ERROR: File not found: {data_path}\")\n",
    "    print(\"Run EDA notebook first to create filtered_complaints.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} complaints\")\n",
    "    print(\"\\nProduct distribution:\")\n",
    "    print(df['Product'].value_counts())\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e868bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create stratified sample\n",
    "def create_sample(df, sample_size=15000):\n",
    "    \"\"\"Create stratified sample by product\"\"\"\n",
    "    print(f\"Creating stratified sample of {sample_size} complaints...\")\n",
    "    \n",
    "    samples = []\n",
    "    for product, group in df.groupby('Product'):\n",
    "        n_samples = max(1, int(sample_size * len(group) / len(df)))\n",
    "        samples.append(group.sample(n=min(n_samples, len(group)), random_state=42))\n",
    "    \n",
    "    sampled_df = pd.concat(samples, ignore_index=True)\n",
    "    print(f\"Sampled {len(sampled_df)} complaints from {len(df)} total\")\n",
    "    print(\"\\nSample distribution:\")\n",
    "    print(sampled_df['Product'].value_counts())\n",
    "    return sampled_df\n",
    "\n",
    "sampled_df = create_sample(df, 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a9dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Text chunking function\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    if not text or len(text) < chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "        \n",
    "        if text_length - start < overlap:\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test chunking\n",
    "test_text = \"This is a test complaint about credit card issues. \" * 30\n",
    "test_chunks = chunk_text(test_text)\n",
    "print(f\"Test: {len(test_text)} characters -> {len(test_chunks)} chunks\")\n",
    "print(f\"Chunk 1 preview: {test_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60839ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create all chunks\n",
    "print(\"Creating chunks for all complaints...\")\n",
    "\n",
    "all_chunks = []\n",
    "all_metadatas = []\n",
    "all_ids = []\n",
    "\n",
    "for idx, row in sampled_df.iterrows():\n",
    "    complaint_id = str(row.get('Complaint ID', idx))\n",
    "    narrative = str(row.get('cleaned_narrative', ''))\n",
    "    \n",
    "    if len(narrative.strip()) < 10:\n",
    "        continue\n",
    "    \n",
    "    chunks = chunk_text(narrative)\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{complaint_id}_{chunk_idx}\"\n",
    "        \n",
    "        metadata = {\n",
    "            'complaint_id': complaint_id,\n",
    "            'product': str(row.get('Product', 'Unknown')),\n",
    "            'date_received': str(row.get('Date received', 'Unknown')),\n",
    "            'chunk_index': chunk_idx,\n",
    "            'total_chunks': len(chunks)\n",
    "        }\n",
    "        \n",
    "        all_chunks.append(chunk)\n",
    "        all_metadatas.append(metadata)\n",
    "        all_ids.append(chunk_id)\n",
    "\n",
    "print(f\"\\nCreated {len(all_chunks)} chunks from {len(sampled_df)} complaints\")\n",
    "print(f\"Average chunks per complaint: {len(all_chunks)/len(sampled_df):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Initialize embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded: {model.get_sentence_embedding_dimension()} dimensions\")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = model.encode([\"Test sentence\"])\n",
    "print(f\"Test embedding shape: {test_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e986ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create embeddings\n",
    "print(f\"Creating embeddings for {len(all_chunks)} chunks...\")\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size = 1000\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(all_chunks), batch_size):\n",
    "    end_idx = min(i + batch_size, len(all_chunks))\n",
    "    batch = all_chunks[i:end_idx]\n",
    "    \n",
    "    embeddings = model.encode(batch, show_progress_bar=False)\n",
    "    all_embeddings.append(embeddings)\n",
    "    \n",
    "    print(f\"Processed batch {i//batch_size + 1}/{(len(all_chunks)+batch_size-1)//batch_size}\")\n",
    "\n",
    "embeddings_array = np.vstack(all_embeddings)\n",
    "print(f\"\\nEmbeddings shape: {embeddings_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39946ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Initialize ChromaDB\n",
    "print(\"Initializing ChromaDB...\")\n",
    "chroma_client = chromadb.PersistentClient(\n",
    "    path=\"../vector_store\",\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "# Create collection\n",
    "collection_name = \"complaints\"\n",
    "try:\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "    print(f\"Using existing collection: {collection_name}\")\n",
    "except:\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\", \"description\": \"CFPB Complaints\"}\n",
    "    )\n",
    "    print(f\"Created new collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c22c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Add to vector store\n",
    "print(f\"Adding {len(all_chunks)} documents to vector store...\")\n",
    "\n",
    "# Add in batches\n",
    "batch_size = 1000\n",
    "for i in range(0, len(all_chunks), batch_size):\n",
    "    end_idx = min(i + batch_size, len(all_chunks))\n",
    "    \n",
    "    collection.add(\n",
    "        embeddings=embeddings_array[i:end_idx].tolist(),\n",
    "        documents=all_chunks[i:end_idx],\n",
    "        metadatas=all_metadatas[i:end_idx],\n",
    "        ids=all_ids[i:end_idx]\n",
    "    )\n",
    "    \n",
    "    print(f\"Added batch {i//batch_size + 1}/{(len(all_chunks)+batch_size-1)//batch_size}\")\n",
    "\n",
    "print(f\"\\nTotal documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ce721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save metadata\n",
    "metadata = {\n",
    "    'total_complaints': len(sampled_df),\n",
    "    'total_chunks': len(all_chunks),\n",
    "    'products_distribution': sampled_df['Product'].value_counts().to_dict(),\n",
    "    'chunk_size': 500,\n",
    "    'chunk_overlap': 50,\n",
    "    'embedding_model': 'all-MiniLM-L6-v2',\n",
    "    'vector_db': 'chromadb'\n",
    "}\n",
    "\n",
    "metadata_path = Path('../vector_store/metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_path}\")\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Complaints processed: {len(sampled_df)}\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"Vector store saved to: ../vector_store/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b03d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Test query\n",
    "test_queries = [\n",
    "    \"credit card payment issues\",\n",
    "    \"loan application problems\",\n",
    "    \"money transfer delays\"\n",
    "]\n",
    "\n",
    "print(\"Testing vector store queries...\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: '{query}'\")\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=2\n",
    "    )\n",
    "    \n",
    "    for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "        print(f\"  Result {i+1}:\")\n",
    "        print(f\"    Product: {metadata.get('product', 'N/A')}\")\n",
    "        print(f\"    Text: {doc[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d1871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
