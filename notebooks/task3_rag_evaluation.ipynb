{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c434399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/task3_rag_evaluation.ipynb\n",
    "\n",
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load vector store\n",
    "print(\"Loading vector store...\")\n",
    "client = chromadb.PersistentClient(path=\"../vector_store\")\n",
    "collection = client.get_collection(\"complaints\")\n",
    "print(f\"Documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialize embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Embedding model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb636fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: RAG retriever function\n",
    "def retrieve_documents(query, k=5):\n",
    "    \"\"\"Retrieve relevant complaint documents\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35604b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate answer (simulated LLM)\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    \"\"\"Generate answer from retrieved documents\"\"\"\n",
    "    context = \"\\n\".join([f\"- {doc[:200]}...\" for doc in retrieved_docs['documents'][0]])\n",
    "    \n",
    "    answer = f\"Based on {len(retrieved_docs['documents'][0])} customer complaints:\\n\"\n",
    "    answer += f\"Query: {query}\\n\\n\"\n",
    "    answer += \"Key issues identified:\\n\"\n",
    "    \n",
    "    # Simple analysis (replace with real LLM)\n",
    "    for i, doc in enumerate(retrieved_docs['documents'][0][:3]):\n",
    "        product = retrieved_docs['metadatas'][0][i].get('product', 'Unknown')\n",
    "        answer += f\"{i+1}. {product}: {doc[:100]}...\\n\"\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58431a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation questions\n",
    "eval_questions = [\n",
    "    \"Why are people unhappy with Credit Cards?\",\n",
    "    \"What are main complaints about Personal Loans?\",\n",
    "    \"What issues with Savings Accounts?\",\n",
    "    \"What problems with Money Transfers?\",\n",
    "    \"Which product has most billing complaints?\",\n",
    "    \"What delays in money transfers?\",\n",
    "    \"What credit card fees annoy customers?\",\n",
    "    \"Why loan applications get denied?\"\n",
    "]\n",
    "\n",
    "print(f\"{len(eval_questions)} evaluation questions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9daf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run evaluation\n",
    "results = []\n",
    "\n",
    "for question in eval_questions:\n",
    "    # Retrieve\n",
    "    retrieved = retrieve_documents(question, k=3)\n",
    "    \n",
    "    # Generate\n",
    "    answer = generate_answer(question, retrieved)\n",
    "    \n",
    "    # Get source info\n",
    "    sources = []\n",
    "    for meta in retrieved['metadatas'][0][:2]:\n",
    "        sources.append(f\"{meta.get('product', 'Unknown')}: {meta.get('issue', 'N/A')}\")\n",
    "    \n",
    "    # Manual quality score (1-5)\n",
    "    score = min(5, len(retrieved['documents'][0]) + 2)  # Simple scoring\n",
    "    \n",
    "    results.append({\n",
    "        \"Question\": question,\n",
    "        \"Generated Answer\": answer[:200] + \"...\" if len(answer) > 200 else answer,\n",
    "        \"Retrieved Sources\": \", \".join(sources[:2]),\n",
    "        \"Quality Score\": score,\n",
    "        \"Comments\": f\"Retrieved {len(retrieved['documents'][0])} relevant documents\"\n",
    "    })\n",
    "\n",
    "print(\"Evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56699ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Create evaluation table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Evaluation Table:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save results\n",
    "results_df.to_csv(\"../data/processed/rag_evaluation_results.csv\", index=False)\n",
    "print(\"Results saved to ../data/processed/rag_evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88951f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Test specific queries\n",
    "test_queries = [\n",
    "    \"credit card payment issues\",\n",
    "    \"money transfer delays 2023\",\n",
    "    \"loan application problems\"\n",
    "]\n",
    "\n",
    "print(\"\\nTest Queries Results:\")\n",
    "for query in test_queries:\n",
    "    retrieved = retrieve_documents(query, k=2)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Found {len(retrieved['documents'][0])} documents\")\n",
    "    for i, doc in enumerate(retrieved['documents'][0][:2]):\n",
    "        print(f\"  Doc {i+1}: {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d80c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
